import streamlit as st
import cv2
import numpy as np
import os
from PIL import Image
from ultralytics import YOLO
import zipfile
import io
import requests
from typing import List, Tuple, Optional, Dict
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import lru_cache
import re
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
import json
import logging
import sys
from datetime import datetime
import asyncio
import aiohttp
import aiofiles
from concurrent.futures import ProcessPoolExecutor
from functools import partial

# Configura√ß√£o do sistema de logging
def setup_logging():
    """Configura o sistema de logging profissional"""
    # Criar diret√≥rio de logs se n√£o existir
    os.makedirs("logs", exist_ok=True)
    
    # Configurar formato dos logs
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        datefmt=date_format,
        handlers=[
            # Arquivo de log com rota√ß√£o di√°ria
            logging.FileHandler(
                f"logs/manhwa_extractor_{datetime.now().strftime('%Y%m%d')}.log",
                encoding='utf-8'
            ),
            # Console output para desenvolvimento
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # Configurar n√≠veis espec√≠ficos para bibliotecas externas
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("PIL").setLevel(logging.WARNING)
    
    return logging.getLogger(__name__)

# Inicializar logging
logger = setup_logging()
logger.info("üöÄ Aplica√ß√£o Extrator de Pain√©is de Manhwa iniciada")

# Sistema de Rate Limiting Inteligente
class RateLimiter:
    """Rate limiter inteligente que n√£o bloqueia a interface"""
    
    def __init__(self):
        self.requests = {}
        self.limits = {
            # Limites espec√≠ficos por dom√≠nio (requests per minute)
            'manhwatop.com': {'requests': 5, 'window': 60},
            'reaperscans.com': {'requests': 3, 'window': 60},
            'asurascans.com': {'requests': 4, 'window': 60},
            'mangadex.org': {'requests': 8, 'window': 60},
            'default': {'requests': 10, 'window': 60}  # Padr√£o para outros sites
        }
        logger.info("üîí Rate Limiter inicializado")
    
    def get_domain(self, url: str) -> str:
        """Extrai dom√≠nio da URL"""
        try:
            return urlparse(url).netloc.lower()
        except:
            return 'unknown'
    
    def can_request(self, url: str) -> tuple[bool, float]:
        """
        Verifica se pode fazer requisi√ß√£o
        Returns: (can_request: bool, wait_time: float)
        """
        domain = self.get_domain(url)
        now = time.time()
        
        # Obter limites para o dom√≠nio
        domain_limits = self.limits.get(domain, self.limits['default'])
        max_requests = domain_limits['requests']
        window = domain_limits['window']
        
        # Inicializar lista de requisi√ß√µes se n√£o existir
        if domain not in self.requests:
            self.requests[domain] = []
        
        # Remover requisi√ß√µes antigas
        self.requests[domain] = [
            req_time for req_time in self.requests[domain]
            if now - req_time < window
        ]
        
        # Verificar se pode fazer requisi√ß√£o
        if len(self.requests[domain]) < max_requests:
            return True, 0.0
        
        # Calcular tempo de espera
        oldest_request = min(self.requests[domain])
        wait_time = window - (now - oldest_request)
        return False, max(0.1, wait_time)
    
    def record_request(self, url: str):
        """Registra uma requisi√ß√£o"""
        domain = self.get_domain(url)
        if domain not in self.requests:
            self.requests[domain] = []
        self.requests[domain].append(time.time())
        logger.debug(f"Rate limit: Requisi√ß√£o registrada para {domain}")
    
    def smart_delay(self, url: str = None, context: str = "general") -> float:
        """
        Calcula delay inteligente baseado no contexto
        Returns: delay em segundos
        """
        if url:
            can_req, wait_time = self.can_request(url)
            if not can_req:
                logger.info(f"Rate limit ativo para {self.get_domain(url)}: aguardando {wait_time:.1f}s")
                return wait_time
        
        # Delays contextuais mais inteligentes
        context_delays = {
            'chapter_processing': 0.1,  # Entre p√°ginas de cap√≠tulo
            'batch_download': 0.2,      # Entre downloads em lote
            'general': 0.05,            # Delay m√≠nimo geral
            'image_processing': 0.0     # Sem delay para processamento local
        }
        
        return context_delays.get(context, 0.1)

# Inicializar rate limiter global
rate_limiter = RateLimiter()

# Sistema de Valida√ß√£o e Sanitiza√ß√£o de Entrada
class InputValidator:
    """Valida√ß√£o robusta de entradas do usu√°rio"""
    
    def __init__(self):
        # Caracteres perigosos para nomes de arquivo
        self.dangerous_chars = r'[<>:"/\\|?*\x00-\x1f\x7f-\x9f]'
        # Padr√µes suspeitos que podem indicar tentativas de inje√ß√£o
        self.suspicious_patterns = [
            r'\.\./',  # Path traversal
            r'<script',  # XSS
            r'javascript:',  # JavaScript injection
            r'data:',  # Data URLs suspeitas
            r'file://',  # File URLs
            r'[<>"]',  # HTML/XML chars
        ]
        logger.info("üõ°Ô∏è Validador de entrada inicializado")
    
    def sanitize_filename(self, filename: str, max_length: int = 100) -> str:
        """
        Sanitiza nome de arquivo removendo caracteres perigosos
        """
        if not isinstance(filename, str):
            logger.warning(f"Nome de arquivo inv√°lido (tipo: {type(filename)})")
            return "arquivo_sem_nome"
        
        # Remove caracteres perigosos
        clean_name = re.sub(self.dangerous_chars, '_', filename)
        
        # Remove m√∫ltiplos underscores consecutivos
        clean_name = re.sub(r'_+', '_', clean_name)
        
        # Remove underscores do in√≠cio e fim
        clean_name = clean_name.strip('_')
        
        # Garante que n√£o est√° vazio
        if not clean_name:
            clean_name = "arquivo_sem_nome"
        
        # Limita o tamanho
        if len(clean_name) > max_length:
            name_part, ext_part = os.path.splitext(clean_name)
            clean_name = name_part[:max_length-len(ext_part)] + ext_part
        
        logger.debug(f"Nome sanitizado: '{filename}' -> '{clean_name}'")
        return clean_name
    
    def validate_user_input(self, text: str, max_length: int = 1000, field_name: str = "entrada") -> str:
        """
        Valida e sanitiza entrada de texto do usu√°rio
        """
        if not isinstance(text, str):
            raise ValueError(f"{field_name} deve ser uma string")
        
        if len(text) > max_length:
            logger.warning(f"{field_name} muito longa ({len(text)} chars), truncando para {max_length}")
            text = text[:max_length]
        
        # Verificar padr√µes suspeitos
        for pattern in self.suspicious_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                logger.warning(f"Padr√£o suspeito detectado em {field_name}: {pattern}")
                raise ValueError(f"Entrada cont√©m caracteres n√£o permitidos")
        
        # Remove caracteres de controle
        clean_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        
        # Remove espa√ßos extras
        clean_text = ' '.join(clean_text.split())
        
        return clean_text
    
    def validate_url_input(self, url: str) -> str:
        """
        Valida√ß√£o espec√≠fica para URLs de entrada do usu√°rio
        """
        if not url:
            raise ValueError("URL n√£o pode estar vazia")
        
        # Sanitizar entrada b√°sica
        url = self.validate_user_input(url, max_length=2000, field_name="URL")
        
        # Verifica√ß√µes espec√≠ficas para URL
        if not url.startswith(('http://', 'https://')):
            raise ValueError("URL deve come√ßar com http:// ou https://")
        
        # Verificar dom√≠nios conhecidos de manhwa
        known_domains = [
            'manhwatop.com', 'reaperscans.com', 'asurascans.com',
            'mangadex.org', 'webtoons.com', 'mangaplus.com'
        ]
        
        domain = urlparse(url).netloc.lower()
        if not any(known in domain for known in known_domains):
            logger.warning(f"Dom√≠nio n√£o reconhecido: {domain}")
            # N√£o bloquear, apenas alertar
        
        return url
    
    def validate_chapter_range(self, start: int, end: int, max_chapters: int = 100) -> tuple[int, int]:
        """
        Valida faixa de cap√≠tulos
        """
        if not isinstance(start, int) or not isinstance(end, int):
            raise ValueError("N√∫meros de cap√≠tulo devem ser inteiros")
        
        if start < 1:
            logger.warning("Cap√≠tulo inicial deve ser >= 1, ajustando")
            start = 1
        
        if end < start:
            logger.warning("Cap√≠tulo final menor que inicial, trocando")
            start, end = end, start
        
        if (end - start + 1) > max_chapters:
            logger.warning(f"Muitos cap√≠tulos solicitados ({end - start + 1}), limitando a {max_chapters}")
            end = start + max_chapters - 1
        
        return start, end

# Inicializar validador
input_validator = InputValidator()

# Sistema de Opera√ß√µes Ass√≠ncronas
class AsyncOperationsManager:
    """Gerenciador de opera√ß√µes ass√≠ncronas para melhor performance"""
    
    def __init__(self):
        self.session = None
        self.semaphore = asyncio.Semaphore(MAX_WORKERS)  # Limite de conex√µes simult√¢neas
        self.process_pool = ProcessPoolExecutor(max_workers=2)  # Para processamento pesado
        logger.info("üöÄ Gerenciador de opera√ß√µes ass√≠ncronas inicializado")
    
    async def get_session(self):
        """Obt√©m ou cria sess√£o HTTP ass√≠ncrona"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
            connector = aiohttp.TCPConnector(limit=MAX_WORKERS, limit_per_host=5)
            self.session = aiohttp.ClientSession(
                headers=SCRAPING_HEADERS,
                timeout=timeout,
                connector=connector
            )
        return self.session
    
    async def close_session(self):
        """Fecha a sess√£o HTTP"""
        if self.session and not self.session.closed:
            await self.session.close()
            logger.info("Sess√£o HTTP fechada")
    
    async def download_image_async(self, url: str) -> Optional[bytes]:
        """Download ass√≠ncrono de imagem com rate limiting"""
        start_time = time.time()
        metrics_collector.active_downloads += 1
        
        try:
            # Verificar rate limiting
            can_request, wait_time = rate_limiter.can_request(url)
            if not can_request:
                logger.info(f"Rate limit ativo para {rate_limiter.get_domain(url)}, aguardando {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
            
            if not validar_url_cached(url):
                metrics_collector.record_request(time.time() - start_time, False, "validation_failed")
                return None
            
            async with self.semaphore:  # Limitar conex√µes simult√¢neas
                session = await self.get_session()
                
                # HEAD request para verificar tamanho
                try:
                    rate_limiter.record_request(url)
                    async with session.head(url) as response:
                        content_length = response.headers.get('content-length')
                        if content_length and int(content_length) > 10 * 1024 * 1024:
                            logger.warning(f"Imagem muito grande ({content_length} bytes), ignorando download")
                            return None
                except Exception:
                    pass  # Continuar com GET se HEAD falhar
                
                # GET request para baixar
                await asyncio.sleep(rate_limiter.smart_delay(url, "image_download"))
                rate_limiter.record_request(url)
                
                async with session.get(url) as response:
                    response.raise_for_status()
                    
                    # Verificar content-type
                    content_type = response.headers.get('content-type', '').lower()
                    if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'webp']):
                        logger.warning(f"Content-type inesperado para imagem: {content_type}")
                        return None
                    
                    # Download com limite de tamanho
                    content = b""
                    max_size = 10 * 1024 * 1024
                    
                    async for chunk in response.content.iter_chunked(8192):
                        content += chunk
                        if len(content) > max_size:
                            logger.warning(f"Tamanho da imagem excedeu {max_size} bytes, truncando download")
                            return None
                    
                    if len(content) < 100:
                        return None
                    
                    logger.debug(f"Download ass√≠ncrono conclu√≠do: {url} ({len(content)} bytes)")
                    metrics_collector.record_request(time.time() - start_time, True, "download_success")
                    return content
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout no download de {url}")
            metrics_collector.record_request(time.time() - start_time, False, "timeout")
            return None
        except aiohttp.ClientError as e:
            logger.error(f"Erro de rede ao baixar imagem de {url}: {e}")
            metrics_collector.record_request(time.time() - start_time, False, "network_error")
            return None
        except Exception as e:
            logger.error(f"Erro inesperado ao baixar imagem de {url}: {e}", exc_info=True)
            metrics_collector.record_request(time.time() - start_time, False, "unexpected_error")
            return None
        finally:
            metrics_collector.active_downloads -= 1
    
    async def download_multiple_images(self, urls: List[str], progress_callback=None) -> List[Tuple[str, Optional[bytes]]]:
        """Download ass√≠ncrono de m√∫ltiplas imagens"""
        logger.info(f"Iniciando download ass√≠ncrono de {len(urls)} imagens")
        
        async def download_with_progress(i, url):
            result = await self.download_image_async(url)
            if progress_callback:
                progress_callback(i + 1, len(urls))
            return url, result
        
        # Executar downloads em paralelo
        tasks = [download_with_progress(i, url) for i, url in enumerate(urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrar resultados v√°lidos
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Erro no download ass√≠ncrono: {result}")
            else:
                valid_results.append(result)
        
        logger.info(f"Download ass√≠ncrono conclu√≠do: {len(valid_results)}/{len(urls)} sucessos")
        return valid_results
    
    async def process_image_async(self, img_data: bytes, nome_fonte: str) -> Tuple[List, Optional[str]]:
        """Processamento ass√≠ncrono de imagem em processo separado"""
        start_time = time.time()
        metrics_collector.active_processing += 1
        
        try:
            logger.info(f"Processando imagem ass√≠ncrona: {nome_fonte} ({len(img_data)} bytes)")
            
            # Executar processamento pesado em processo separado
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.process_pool,
                self._process_image_sync,
                img_data,
                nome_fonte
            )
            
            # Registrar m√©tricas de processamento
            paineis, erro = result
            processing_time = time.time() - start_time
            panels_count = len(paineis) if not erro else 0
            metrics_collector.record_processing(processing_time, panels_count, nome_fonte)
            
            return result
        except Exception as e:
            logger.error(f"Erro no processamento ass√≠ncrono de {nome_fonte}: {e}", exc_info=True)
            processing_time = time.time() - start_time
            metrics_collector.record_processing(processing_time, 0, nome_fonte)
            return [], str(e)
        finally:
            metrics_collector.active_processing -= 1
    
    def _process_image_sync(self, img_data: bytes, nome_fonte: str) -> Tuple[List, Optional[str]]:
        """Processamento s√≠ncrono de imagem (executado em processo separado)"""
        try:
            img_pil = carregar_e_redimensionar_imagem(img_data)
            if img_pil is None:
                logger.warning(f"Falha ao carregar imagem: {nome_fonte}")
                return [], "Erro ao carregar imagem"
                
            img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
            paineis = extrair_paineis_hibrido_otimizado(img, img_id=nome_fonte)
            logger.info(f"Extra√≠dos {len(paineis)} pain√©is de {nome_fonte}")
            return paineis, None
        except Exception as e:
            logger.error(f"Erro no processamento s√≠ncrono de {nome_fonte}: {e}")
            return [], str(e)
    
    def cleanup(self):
        """Limpeza de recursos"""
        if self.process_pool:
            self.process_pool.shutdown(wait=True)
            logger.info("Process pool encerrado")

# Inicializar gerenciador ass√≠ncrono
async_manager = AsyncOperationsManager()

# Sistema de Monitoramento e M√©tricas
from dataclasses import dataclass, field
from collections import deque
import psutil

@dataclass
class PerformanceMetrics:
    """M√©tricas de performance da aplica√ß√£o"""
    total_requests: int = 0
    failed_requests: int = 0
    total_images_processed: int = 0
    total_panels_extracted: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_download_time: float = 0.0
    avg_processing_time: float = 0.0
    peak_memory_mb: float = 0.0
    start_time: float = field(default_factory=time.time)
    response_times: deque = field(default_factory=lambda: deque(maxlen=100))
    processing_times: deque = field(default_factory=lambda: deque(maxlen=100))

class MetricsCollector:
    """Coletor de m√©tricas em tempo real"""
    
    def __init__(self):
        self.metrics = PerformanceMetrics()
        self.active_downloads = 0
        self.active_processing = 0
        logger.info("üìä Sistema de m√©tricas inicializado")
    
    def record_request(self, duration: float, success: bool, request_type: str = "download"):
        """Registra m√©tricas de requisi√ß√£o"""
        self.metrics.total_requests += 1
        if not success:
            self.metrics.failed_requests += 1
        
        self.metrics.response_times.append(duration)
        if self.metrics.response_times:
            self.metrics.avg_download_time = sum(self.metrics.response_times) / len(self.metrics.response_times)
        
        logger.debug(f"M√©trica registrada: {request_type} - {duration:.2f}s - {'‚úÖ' if success else '‚ùå'}")
    
    def record_processing(self, duration: float, panels_extracted: int, image_name: str):
        """Registra m√©tricas de processamento"""
        self.metrics.total_images_processed += 1
        self.metrics.total_panels_extracted += panels_extracted
        
        self.metrics.processing_times.append(duration)
        if self.metrics.processing_times:
            self.metrics.avg_processing_time = sum(self.metrics.processing_times) / len(self.metrics.processing_times)
        
        logger.debug(f"Processamento registrado: {image_name} - {duration:.2f}s - {panels_extracted} pain√©is")
    
    def record_cache_event(self, hit: bool):
        """Registra evento de cache"""
        if hit:
            self.metrics.cache_hits += 1
        else:
            self.metrics.cache_misses += 1
    
    def update_memory_usage(self):
        """Atualiza uso de mem√≥ria"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            if memory_mb > self.metrics.peak_memory_mb:
                self.metrics.peak_memory_mb = memory_mb
        except Exception:
            pass  # psutil pode n√£o estar dispon√≠vel
    
    def get_success_rate(self) -> float:
        """Calcula taxa de sucesso"""
        if self.metrics.total_requests == 0:
            return 0.0
        return (self.metrics.total_requests - self.metrics.failed_requests) / self.metrics.total_requests
    
    def get_cache_hit_rate(self) -> float:
        """Calcula taxa de acerto do cache"""
        total = self.metrics.cache_hits + self.metrics.cache_misses
        if total == 0:
            return 0.0
        return self.metrics.cache_hits / total
    
    def get_uptime(self) -> float:
        """Calcula tempo de execu√ß√£o em segundos"""
        return time.time() - self.metrics.start_time
    
    def get_throughput(self) -> float:
        """Calcula throughput (pain√©is por minuto)"""
        uptime_minutes = self.get_uptime() / 60
        if uptime_minutes == 0:
            return 0.0
        return self.metrics.total_panels_extracted / uptime_minutes
    
    def display_dashboard(self):
        """Exibe dashboard completo de m√©tricas"""
        st.markdown("### üìä Dashboard de Performance")
        
        # M√©tricas principais
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Taxa de Sucesso", 
                f"{self.get_success_rate():.1%}",
                delta=f"{self.metrics.total_requests} reqs" if self.metrics.total_requests > 0 else None
            )
        
        with col2:
            st.metric(
                "Pain√©is Extra√≠dos", 
                self.metrics.total_panels_extracted,
                delta=f"{self.metrics.total_images_processed} imgs"
            )
        
        with col3:
            st.metric(
                "Cache Hit Rate", 
                f"{self.get_cache_hit_rate():.1%}",
                delta="√ìtimo" if self.get_cache_hit_rate() > 0.8 else "Baixo"
            )
        
        with col4:
            st.metric(
                "Throughput", 
                f"{self.get_throughput():.1f}/min",
                delta=f"{self.get_uptime()/60:.1f}min uptime"
            )
        
        # M√©tricas de tempo
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "Tempo M√©dio Download", 
                f"{self.metrics.avg_download_time:.2f}s",
                delta="R√°pido" if self.metrics.avg_download_time < 2.0 else "Lento"
            )
        
        with col2:
            st.metric(
                "Tempo M√©dio Processamento", 
                f"{self.metrics.avg_processing_time:.2f}s",
                delta="Eficiente" if self.metrics.avg_processing_time < 5.0 else "Lento"
            )
        
        with col3:
            self.update_memory_usage()
            st.metric(
                "Pico de Mem√≥ria", 
                f"{self.metrics.peak_memory_mb:.1f}MB",
                delta="Normal" if self.metrics.peak_memory_mb < 500 else "Alto"
            )
        
        # Opera√ß√µes em tempo real
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric(
                "Downloads Ativos", 
                self.active_downloads,
                help="N√∫mero de downloads em execu√ß√£o"
            )
        
        with col2:
            st.metric(
                "Processamentos Ativos", 
                self.active_processing,
                help="N√∫mero de imagens sendo processadas"
            )
        
        # Gr√°fico de tempos de resposta (√∫ltimos 50)
        if len(self.metrics.response_times) > 5:
            st.markdown("#### üìà Tempos de Resposta Recentes")
            chart_data = list(self.metrics.response_times)[-50:]
            st.line_chart(chart_data)
        
        # Alertas autom√°ticos
        self._show_performance_alerts()
    
    def _show_performance_alerts(self):
        """Mostra alertas de performance"""
        alerts = []
        
        if self.get_success_rate() < 0.8 and self.metrics.total_requests > 10:
            alerts.append("üî¥ Taxa de sucesso baixa (<80%)")
        
        if self.metrics.avg_download_time > 5.0:
            alerts.append("üü° Downloads lentos (>5s)")
        
        if self.metrics.avg_processing_time > 10.0:
            alerts.append("üü° Processamento lento (>10s)")
        
        if self.metrics.peak_memory_mb > 1000:
            alerts.append("üî¥ Alto uso de mem√≥ria (>1GB)")
        
        if self.get_cache_hit_rate() < 0.3 and (self.metrics.cache_hits + self.metrics.cache_misses) > 10:
            alerts.append("üü° Cache pouco eficiente (<30%)")
        
        if alerts:
            st.markdown("#### ‚ö†Ô∏è Alertas de Performance")
            for alert in alerts:
                st.warning(alert)
    
    def get_summary_stats(self) -> Dict:
        """Retorna estat√≠sticas resumidas"""
        return {
            "success_rate": self.get_success_rate(),
            "total_panels": self.metrics.total_panels_extracted,
            "avg_download": self.metrics.avg_download_time,
            "avg_processing": self.metrics.avg_processing_time,
            "cache_rate": self.get_cache_hit_rate(),
            "throughput": self.get_throughput(),
            "uptime": self.get_uptime(),
            "memory_peak": self.metrics.peak_memory_mb
        }

# Inicializar coletor de m√©tricas
metrics_collector = MetricsCollector()

# Wrapper para executar opera√ß√µes ass√≠ncronas no Streamlit
def run_async(coro):
    """Executa corrotina ass√≠ncrona de forma compat√≠vel com Streamlit"""
    try:
        # Tentar usar loop existente
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # Se j√° h√° um loop rodando, criar novo thread
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, coro)
                return future.result()
        else:
            return loop.run_until_complete(coro)
    except RuntimeError:
        # Criar novo loop se necess√°rio
        return asyncio.run(coro)

def process_chapter_async(capitulo_info: Dict, progress_container=None) -> List[Tuple[Image.Image, str]]:
    """Vers√£o ass√≠ncrona do processamento de cap√≠tulo"""
    
    async def _process_async():
        urls_imagens = extrair_imagens_capitulo(capitulo_info["url"])
        
        if not urls_imagens:
            return []
        
        paineis_capitulo = []
        
        # Callback de progresso thread-safe
        def update_progress(current, total):
            if progress_container:
                progress_container.progress(current / total, 
                                          f"Processando p√°gina {current}/{total}")
        
        # Download ass√≠ncrono de m√∫ltiplas imagens
        download_results = await async_manager.download_multiple_images(
            urls_imagens, 
            progress_callback=update_progress
        )
        
        # Processar imagens em paralelo
        tasks = []
        for i, (url, img_data) in enumerate(download_results):
            if img_data:
                task = async_manager.process_image_async(
                    img_data, 
                    f"cap_{capitulo_info['numero']}_pag_{i+1}"
                )
                tasks.append(task)
        
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Erro no processamento ass√≠ncrono: {result}")
                else:
                    paineis, erro = result
                    if not erro:
                        paineis_capitulo.extend(paineis)
        
        return paineis_capitulo
    
    return run_async(_process_async())

# Fun√ß√£o auxiliar para delays n√£o-bloqueantes
def smart_sleep(duration: float = None, context: str = "general", url: str = None, show_progress: bool = True):
    """
    Implementa delay inteligente sem bloquear completamente a interface
    """
    if duration is None:
        duration = rate_limiter.smart_delay(url, context)
    
    if duration <= 0:
        return
    
    logger.debug(f"Smart sleep: {duration:.2f}s (contexto: {context})")
    
    if show_progress and duration > 0.5:
        # Para delays longos, mostrar progresso
        progress_placeholder = st.empty()
        steps = max(1, int(duration * 10))  # 10 steps per second
        step_duration = duration / steps
        
        for i in range(steps):
            remaining = duration - (i * step_duration)
            progress_placeholder.info(f"‚è≥ Aguardando {remaining:.1f}s para n√£o sobrecarregar o servidor...")
            time.sleep(step_duration)
        
        progress_placeholder.empty()
    else:
        # Para delays curtos, sleep simples
        time.sleep(duration)

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Extrator de Pain√©is de Manhwa", layout="wide")
st.title("üñºÔ∏è Extrator de Pain√©is de Manhwa")
st.markdown('<div id="topo"></div>', unsafe_allow_html=True)

# Constantes otimizadas
MAX_WIDTH = 1024
MIN_CONTOUR_SIZE = 100
REQUEST_TIMEOUT = 15
Y_TOLERANCE = 40
PREVIEW_LIMIT = 20
COLS_PER_ROW = 4
MAX_WORKERS = 4
BATCH_SIZE = 10

# Lock para thread safety
thread_lock = threading.Lock()

# Headers para web scraping
SCRAPING_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none"
}

# Caminho onde o modelo ser√° salvo
MODELO_PATH = "modelos/best.pt"

# URL do modelo no Dropbox (alterado para download direto)
URL_MODELO_DROPBOX = "https://www.dropbox.com/scl/fi/a743aqjqzau3fxy4fss4a/best.pt?rlkey=a24lozm0cw8znku0h743ylx2z&dl=1"

def baixar_modelo_yolo(dropbox_url, destino=MODELO_PATH):
    if not os.path.exists(destino):
        try:
            logger.info("üì¶ Iniciando download do modelo YOLO (best.pt)...")
            os.makedirs(os.path.dirname(destino), exist_ok=True)
            response = requests.get(dropbox_url, stream=True)
            response.raise_for_status()
            with open(destino, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            logger.info("‚úÖ Modelo YOLO baixado com sucesso!")
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar modelo YOLO: {e}", exc_info=True)

def carregar_modelo():
    try:
        modelo = YOLO(MODELO_PATH)
        return modelo
    except Exception as e:
        logger.error(f"‚ùå Erro ao carregar o modelo YOLO: {e}", exc_info=True)
        return None

# Executar download e carregamento
baixar_modelo_yolo(URL_MODELO_DROPBOX)
model = carregar_modelo()

# Teste simples (evite executar na nuvem, s√≥ para checagem)
if model:
    logger.info("‚úÖ Modelo YOLO carregado com sucesso!")
else:
    logger.warning("‚ö†Ô∏è Modelo YOLO n√£o foi carregado - usando fallback para contornos")

# Inicializa√ß√£o do estado da sess√£o
def init_session_state():
    defaults = {
        "contador_paineis": 0,
        "painel_coletor": [],
        "imagens_processadas": set(),
        "paineis_processados": set(),
        "_cache_hash": {},
        "capitulos_cache": {},  # Cache para cap√≠tulos
        "manhwa_info": {}  # Informa√ß√µes do manhwa
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state()

# Fun√ß√µes auxiliares otimizadas (mantendo as originais)
@st.cache_data(show_spinner=False, max_entries=50)
def carregar_e_redimensionar_imagem(file_data: bytes, max_width=MAX_WIDTH):
    try:
        img = Image.open(io.BytesIO(file_data)).convert("RGB")
        if img.width > max_width:
            ratio = max_width / img.width
            new_size = (max_width, int(img.height * ratio))
            img = img.resize(new_size, Image.NEAREST if ratio < 0.5 else Image.LANCZOS)
        return img
    except Exception as e:
        st.error(f"Erro ao carregar imagem: {e}")
        return None

@lru_cache(maxsize=1000)
def calcular_hash_rapido(data_bytes: bytes) -> str:
    if len(data_bytes) < 1024:
        return hashlib.md5(data_bytes).hexdigest()
    
    sample = data_bytes[:512] + data_bytes[-512:] + str(len(data_bytes)).encode()
    return hashlib.md5(sample).hexdigest()

def calcular_hash_imagem_otimizado(img_pil: Image.Image) -> str:
    thumb = img_pil.copy()
    thumb.thumbnail((32, 32), Image.NEAREST)
    
    buf = io.BytesIO()
    thumb.save(buf, format='PNG', optimize=True)
    return calcular_hash_rapido(buf.getvalue())

def ordenar_paineis_otimizado(paineis: List[dict], y_tol: int = Y_TOLERANCE) -> List[dict]:
    if not paineis:
        return []
    
    try:
        y_coords = np.array([p["y"] for p in paineis])
        x_coords = np.array([p["x"] for p in paineis])
        sorted_indices = np.lexsort((x_coords, y_coords))
        return [paineis[i] for i in sorted_indices]
    except (KeyError, ValueError, IndexError, TypeError) as e:
        # Fallback to original sorting if numpy operations fail
        logger.warning(f"Ordena√ß√£o otimizada falhou ({e}), usando fallback para ordena√ß√£o original")
        return ordenar_paineis_original(paineis, y_tol)

def ordenar_paineis_original(paineis: List[dict], y_tol: int = Y_TOLERANCE) -> List[dict]:
    linhas = []
    for p in sorted(paineis, key=lambda p: p["y"]):
        colocado = False
        for linha in linhas:
            if abs(p["y"] - linha[0]["y"]) < y_tol:
                linha.append(p)
                colocado = True
                break
        if not colocado:
            linhas.append([p])
    
    ordenados = []
    for linha in linhas:
        ordenados.extend(sorted(linha, key=lambda p: p["x"]))
    return ordenados

def extrair_paineis_yolo_otimizado(img) -> List[dict]:
    if model is None:
        return []
    
    painel_infos = []
    try:
        results = model(img, verbose=False, conf=0.3, iou=0.5)
        
        for r in results:
            if r.boxes is None:
                continue
                
            boxes = r.boxes.xyxy.cpu().numpy()
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)
                
                w, h = x2 - x1, y2 - y1
                if w < MIN_CONTOUR_SIZE or h < MIN_CONTOUR_SIZE:
                    continue
                
                painel = img[y1:y2, x1:x2]
                if painel.size > 0:
                    painel_rgb = cv2.cvtColor(painel, cv2.COLOR_BGR2RGB)
                    img_pil = Image.fromarray(painel_rgb)
                    painel_infos.append({"x": x1, "y": y1, "img": img_pil})
    except Exception as e:
        st.warning(f"Erro no YOLO: {e}")
        return []
    
    return painel_infos

def extrair_paineis_contorno_otimizado(img) -> List[dict]:
    painel_infos = []
    try:
        h, w = img.shape[:2]
        scale = 1.0
        if w > 2000 or h > 2000:
            scale = min(2000/w, 2000/h)
            new_w, new_h = int(w * scale), int(h * scale)
            img_small = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
        else:
            img_small = img
        
        gray = cv2.cvtColor(img_small, cv2.COLOR_BGR2GRAY)
        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY_INV, 9, 2)
        
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        min_size = int(MIN_CONTOUR_SIZE * scale)
        
        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)
            if w > min_size and h > min_size:
                if scale != 1.0:
                    x, y, w, h = int(x/scale), int(y/scale), int(w/scale), int(h/scale)
                
                painel = img[y:y + h, x:x + w]
                if painel.size > 0:
                    painel_rgb = cv2.cvtColor(painel, cv2.COLOR_BGR2RGB)
                    img_pil = Image.fromarray(painel_rgb)
                    painel_infos.append({"x": x, "y": y, "img": img_pil})
    except Exception as e:
        st.warning(f"Erro na detec√ß√£o por contorno: {e}")
        return []
    
    return painel_infos

def extrair_paineis_hibrido_otimizado(img, yolo_threshold=1, img_id=None) -> List[Tuple[Image.Image, str]]:
    painel_infos = extrair_paineis_yolo_otimizado(img)
    
    if len(painel_infos) < yolo_threshold:
        painel_infos = extrair_paineis_contorno_otimizado(img)
    
    ordenados = ordenar_paineis_otimizado(painel_infos)
    resultados = []
    
    with thread_lock:
        for painel in ordenados:
            hash_painel = calcular_hash_imagem_otimizado(painel["img"])
            
            if hash_painel not in st.session_state.paineis_processados:
                st.session_state.contador_paineis += 1
                nome_base = f"painel_{st.session_state.contador_paineis:06}.png"
                # Sanitizar nome do arquivo para seguran√ßa
                nome_arquivo = input_validator.sanitize_filename(nome_base)
                resultados.append((painel["img"], nome_arquivo))
                st.session_state.paineis_processados.add(hash_painel)
    
    return resultados

def processar_imagem_otimizada(img_data: bytes, nome_fonte: str) -> Tuple[List, Optional[str]]:
    try:
        logger.info(f"Processando imagem: {nome_fonte} ({len(img_data)} bytes)")
        img_pil = carregar_e_redimensionar_imagem(img_data)
        if img_pil is None:
            logger.warning(f"Falha ao carregar imagem: {nome_fonte}")
            return [], "Erro ao carregar imagem"
            
        img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        paineis = extrair_paineis_hibrido_otimizado(img, img_id=nome_fonte)
        logger.info(f"Extra√≠dos {len(paineis)} pain√©is de {nome_fonte}")
        return paineis, None
    except Exception as e:
        logger.error(f"Erro ao processar imagem {nome_fonte}: {e}", exc_info=True)
        return [], str(e)

@lru_cache(maxsize=100)
def validar_url_cached(url: str) -> bool:
    url_pattern = re.compile(
        r'^https?://'
        r'(?:\S+(?::\S*)?@)?'
        r'(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)*'
        r'[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?'
        r'(?::[0-9]{1,5})?'
        r'(?:/\S*)?$', re.IGNORECASE)
    
    if not url_pattern.match(url):
        return False
    
    # Additional security checks to prevent SSRF
    try:
        parsed = urlparse(url)
        hostname = parsed.hostname
        
        # Block localhost and internal IPs
        if hostname in ['localhost', '127.0.0.1', '0.0.0.0']:
            return False
        
        # Block private IP ranges
        import ipaddress
        try:
            ip = ipaddress.ip_address(hostname)
            if ip.is_private or ip.is_loopback or ip.is_reserved:
                return False
        except (ValueError, ipaddress.AddressValueError):
            # Not an IP address, likely a domain name - continue validation
            pass
        
        # Block suspicious ports
        port = parsed.port
        if port and port in [22, 23, 25, 53, 135, 139, 445, 993, 995]:
            return False
            
        return True
    except Exception:
        return False

def baixar_imagem_url_otimizada(url: str) -> Optional[bytes]:
    try:
        if not validar_url_cached(url):
            return None
        
        # Verificar rate limiting antes da requisi√ß√£o
        can_request, wait_time = rate_limiter.can_request(url)
        if not can_request:
            logger.info(f"Rate limit ativo para {rate_limiter.get_domain(url)}, aguardando {wait_time:.1f}s")
            smart_sleep(wait_time, context="rate_limit", url=url, show_progress=False)
            
        # First, make a HEAD request to check content-length
        try:
            rate_limiter.record_request(url)  # Registrar requisi√ß√£o HEAD
            head_response = requests.head(url, headers=SCRAPING_HEADERS, timeout=REQUEST_TIMEOUT)
            content_length = head_response.headers.get('content-length')
            if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MB limit
                logger.warning(f"Imagem muito grande ({content_length} bytes), ignorando download")
                return None
        except (requests.RequestException, ValueError):
            # If HEAD request fails, continue with GET but be more cautious
            pass
        
        # Pequeno delay inteligente antes do GET
        smart_sleep(context="image_download", url=url, show_progress=False)
        
        rate_limiter.record_request(url)  # Registrar requisi√ß√£o GET
        response = requests.get(url, headers=SCRAPING_HEADERS, timeout=REQUEST_TIMEOUT, stream=True)
        response.raise_for_status()
        
        # Verify content type
        content_type = response.headers.get('content-type', '').lower()
        if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'webp']):
            logger.warning(f"Content-type inesperado para imagem: {content_type}")
            return None
        
        content = b""
        max_size = 10 * 1024 * 1024  # Reduced to 10MB for safety
        
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:  # Filter out keep-alive chunks
                content += chunk
                if len(content) > max_size:
                    logger.warning(f"Tamanho da imagem excedeu {max_size} bytes, truncando download")
                    response.close()
                    return None
        
        # Final validation - ensure it's actually an image
        if len(content) < 100:  # Too small to be a valid image
            return None
            
        return content
    except requests.RequestException as e:
        logger.error(f"Erro de rede ao baixar imagem de {url}: {e}")
        return None
    except Exception as e:
        logger.error(f"Erro inesperado ao baixar imagem de {url}: {e}", exc_info=True)
        return None

# NOVAS FUN√á√ïES PARA WEB SCRAPING

@st.cache_data(ttl=300, show_spinner=False)  # Cache por 5 minutos
def fazer_requisicao_web(url: str) -> Optional[BeautifulSoup]:
    """Faz requisi√ß√£o web e retorna BeautifulSoup"""
    try:
        logger.info(f"Fazendo requisi√ß√£o para: {url}")
        
        # Aplicar rate limiting inteligente
        can_request, wait_time = rate_limiter.can_request(url)
        if not can_request:
            logger.info(f"Rate limit ativo para {rate_limiter.get_domain(url)}, aguardando {wait_time:.1f}s")
            smart_sleep(wait_time, context="web_scraping", url=url)
        
        rate_limiter.record_request(url)
        response = requests.get(url, headers=SCRAPING_HEADERS, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        logger.info(f"Requisi√ß√£o bem-sucedida para {url} (status: {response.status_code})")
        return BeautifulSoup(response.content, 'html.parser')
    except Exception as e:
        logger.error(f"Erro ao acessar URL {url}: {e}")
        st.error(f"‚ùå Erro ao acessar URL: {e}")
        return None

def extrair_info_manhwa_manhwatop(soup: BeautifulSoup, base_url: str) -> Dict:
    """Extrai informa√ß√µes do manhwa do ManhwaTop"""
    logger.info(f"Extraindo informa√ß√µes do manhwa de {base_url}")
    info = {
        "titulo": "Manhwa",
        "capa": None,
        "sinopse": "",
        "capitulos": []
    }
    
    try:
        # T√≠tulo
        titulo_elem = soup.find('h1') or soup.find('h2') or soup.find('.manga-title')
        if titulo_elem:
            info["titulo"] = titulo_elem.get_text(strip=True)
        
        # Capa
        capa_elem = soup.find('img', class_=re.compile(r'manga.*cover|cover.*manga|wp-post-image'))
        if not capa_elem:
            capa_elem = soup.find('img', src=re.compile(r'cover|thumbnail'))
        if capa_elem and capa_elem.get('src'):
            info["capa"] = urljoin(base_url, capa_elem['src'])
        
        # Sinopse
        sinopse_elem = soup.find('div', class_=re.compile(r'summary|description|synopsis'))
        if sinopse_elem:
            info["sinopse"] = sinopse_elem.get_text(strip=True)[:200] + "..."
        
        # Cap√≠tulos - m√∫ltiplos seletores para diferentes layouts
        capitulos_containers = [
            soup.find_all('li', class_=re.compile(r'wp-manga-chapter')),
            soup.find_all('a', href=re.compile(r'chapter|cap')),
            soup.find_all('div', class_=re.compile(r'chapter'))
        ]
        
        capitulos_links = set()  # Usar set para evitar duplicatas
        
        for container in capitulos_containers:
            for elem in container:
                link = None
                titulo = ""
                
                if elem.name == 'a':
                    link = elem.get('href')
                    titulo = elem.get_text(strip=True)
                elif elem.name in ['li', 'div']:
                    link_elem = elem.find('a')
                    if link_elem:
                        link = link_elem.get('href')
                        titulo = link_elem.get_text(strip=True)
                
                if link and 'chapter' in link.lower():
                    full_url = urljoin(base_url, link)
                    if full_url not in capitulos_links:
                        capitulos_links.add(full_url)
                        # Extrair n√∫mero do cap√≠tulo
                        numero_match = re.search(r'chapter[^\d]*(\d+(?:\.\d+)?)', titulo.lower() + ' ' + link.lower())
                        numero = numero_match.group(1) if numero_match else str(len(info["capitulos"]) + 1)
                        
                        info["capitulos"].append({
                            "numero": numero,
                            "titulo": titulo or f"Cap√≠tulo {numero}",
                            "url": full_url
                        })
        
        # Ordenar cap√≠tulos por n√∫mero
        info["capitulos"].sort(key=lambda x: float(x["numero"]) if x["numero"].replace('.', '').isdigit() else 999)
        logger.info(f"Extra√≠dos {len(info['capitulos'])} cap√≠tulos do manhwa '{info['titulo']}'")
        
    except Exception as e:
        logger.error(f"Erro ao extrair informa√ß√µes do manhwa: {e}", exc_info=True)
        st.error(f"Erro ao extrair informa√ß√µes: {e}")
    
    return info

def extrair_imagens_capitulo(url_capitulo: str) -> List[str]:
    """Extrai URLs das imagens de um cap√≠tulo"""
    soup = fazer_requisicao_web(url_capitulo)
    if not soup:
        return []
    
    imagens = []
    base_url = f"{urlparse(url_capitulo).scheme}://{urlparse(url_capitulo).netloc}"
    
    # M√∫ltiplos seletores para encontrar imagens do cap√≠tulo
    seletores_img = [
        'div.reading-content img',
        'div.page-break img',
        'div.wp-manga-chapter-img img',
        'img[src*="wp-content"]',
        'img[data-src]',
        '.chapter-content img'
    ]
    
    urls_encontradas = set()
    
    for seletor in seletores_img:
        elementos = soup.select(seletor)
        for img in elementos:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                url_completa = urljoin(base_url, src)
                if any(ext in url_completa.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']) and url_completa not in urls_encontradas:
                    urls_encontradas.add(url_completa)
                    imagens.append(url_completa)
    
    return imagens

def processar_capitulo_completo(capitulo_info: Dict) -> List[Tuple[Image.Image, str]]:
    """Processa um cap√≠tulo completo e extrai todos os pain√©is (vers√£o otimizada ass√≠ncrona)"""
    progress_container = st.empty()
    
    try:
        # Usar vers√£o ass√≠ncrona para melhor performance
        paineis_capitulo = process_chapter_async(capitulo_info, progress_container)
        logger.info(f"Processamento ass√≠ncrono do cap√≠tulo {capitulo_info['numero']} conclu√≠do: {len(paineis_capitulo)} pain√©is")
        return paineis_capitulo
    except Exception as e:
        logger.error(f"Erro no processamento ass√≠ncrono do cap√≠tulo {capitulo_info['numero']}: {e}")
        st.error(f"Erro no processamento: {e}")
        return []
    finally:
        progress_container.empty()

def mostrar_paineis_grid_otimizado(paineis: List[Tuple], titulo: str, expandido: bool = True):
    with st.expander(f"{titulo} - {len(paineis)} painel(is)", expanded=expandido):
        if not paineis:
            st.warning("‚ö†Ô∏è Nenhum painel detectado.")
            return
        
        limite_exibicao = min(12, len(paineis))
        paineis_exibir = paineis[:limite_exibicao]
        
        cols = st.columns(COLS_PER_ROW)
        for i, (painel_img, _) in enumerate(paineis_exibir):
            with cols[i % COLS_PER_ROW]:
                preview_img = painel_img.copy()
                preview_img.thumbnail((200, 200), Image.NEAREST)
                st.image(preview_img, use_container_width=True)
                st.caption(f"Painel {i+1}")
        
        if len(paineis) > limite_exibicao:
            st.info(f"Mostrando {limite_exibicao} de {len(paineis)} pain√©is")

def criar_zip_otimizado(paineis: List[Tuple] = None) -> bytes:
    paineis_usar = paineis or st.session_state.painel_coletor
    if not paineis_usar:
        return b""
    
    zip_buffer = io.BytesIO()
    try:
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
            for img_pil, nome_arquivo in paineis_usar:
                buf = io.BytesIO()
                img_pil.save(buf, format="PNG", optimize=True, compress_level=6)
                zipf.writestr(nome_arquivo, buf.getvalue())
        return zip_buffer.getvalue()
    except Exception as e:
        st.error(f"Erro ao criar ZIP: {e}")
        return b""

def reset_processamento():
    st.session_state.painel_coletor.clear()
    st.session_state.contador_paineis = 0
    st.session_state.imagens_processadas.clear()
    st.session_state.paineis_processados.clear()
    st.session_state._cache_hash.clear()
    st.session_state.capitulos_cache.clear()
    st.session_state.manhwa_info.clear()
    
    calcular_hash_rapido.cache_clear()
    validar_url_cached.cache_clear()

# Interface principal
st.sidebar.markdown("### üìä Estat√≠sticas")
st.sidebar.metric("Pain√©is extra√≠dos", len(st.session_state.painel_coletor))
st.sidebar.metric("Imagens processadas", len(st.session_state.imagens_processadas))

# Sistema de logs na sidebar
st.sidebar.markdown("### üìã Logs do Sistema")
if st.sidebar.checkbox("üîç Mostrar Logs", help="Exibe logs em tempo real"):
    log_level = st.sidebar.selectbox(
        "N√≠vel de Log:",
        ["DEBUG", "INFO", "WARNING", "ERROR"],
        index=1
    )
    
    # Configurar n√≠vel de log dinamicamente
    numeric_level = getattr(logging, log_level.upper())
    logger.setLevel(numeric_level)
    
    st.sidebar.info(f"üìä N√≠vel atual: {log_level}")
    
    # Mostrar √∫ltimas linhas do log
    try:
        from pathlib import Path
        log_files = list(Path("logs").glob("manhwa_extractor_*.log"))
        if log_files:
            latest_log = max(log_files, key=lambda x: x.stat().st_mtime)
            with open(latest_log, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if lines:
                    st.sidebar.text_area(
                        "√öltimos logs:",
                        ''.join(lines[-10:]),  # √öltimas 10 linhas
                        height=200
                    )
    except Exception as e:
        st.sidebar.error(f"Erro ao ler logs: {e}")

# Bot√µes de controle
col1, col2 = st.sidebar.columns(2)
with col1:
    if st.button("üóëÔ∏è Limpar", type="secondary", use_container_width=True):
        reset_processamento()
        st.sidebar.success("Tudo limpo!")

with col2:
    if st.button("üîÑ Reset", type="secondary", use_container_width=True):
        reset_processamento()
        st.rerun()

# Rate Limiter status
st.sidebar.markdown("### üîí Rate Limiter")
if rate_limiter.requests:
    for domain, requests_list in rate_limiter.requests.items():
        if requests_list:  # S√≥ mostrar dom√≠nios com requisi√ß√µes recentes
            recent_requests = len([r for r in requests_list if time.time() - r < 60])
            limit = rate_limiter.limits.get(domain, rate_limiter.limits['default'])['requests']
            
            # Indicador visual do status
            if recent_requests >= limit * 0.8:
                status = "üî¥"  # Pr√≥ximo do limite
            elif recent_requests >= limit * 0.5:
                status = "üü°"  # Uso moderado
            else:
                status = "üü¢"  # Ok
            
            st.sidebar.metric(
                f"{status} {domain}",
                f"{recent_requests}/{limit}",
                help=f"Requisi√ß√µes na √∫ltima hora"
            )
else:
    st.sidebar.info("üü¢ Nenhuma requisi√ß√£o recente")

# Monitoramento de opera√ß√µes ass√≠ncronas
st.sidebar.markdown("### ‚ö° Opera√ß√µes Ass√≠ncronas")
if hasattr(async_manager, 'session') and async_manager.session:
    if async_manager.session.closed:
        status = "üî¥ Sess√£o fechada"
    else:
        status = "üü¢ Sess√£o ativa"
    st.sidebar.metric("Status HTTP", status)
    
    # Mostrar estat√≠sticas do sem√°foro
    if hasattr(async_manager, 'semaphore'):
        available = async_manager.semaphore._value
        total = MAX_WORKERS
        used = total - available
        st.sidebar.metric(
            "Conex√µes simult√¢neas", 
            f"{used}/{total}",
            help="N√∫mero de downloads em paralelo"
        )
else:
    st.sidebar.info("üí§ Sess√£o ass√≠ncrona inativa")

# Debug info
if st.sidebar.checkbox("üîç Debug"):
    st.sidebar.write(f"Pain√©is √∫nicos: {len(st.session_state.paineis_processados)}")
    st.sidebar.write(f"Cache size: {len(st.session_state._cache_hash)}")
    st.sidebar.write(f"Manhwas cache: {len(st.session_state.capitulos_cache)}")

# Abas principais
tab1, tab2, tab3, tab4, tab5 = st.tabs(["üñºÔ∏è Extrair Pain√©is", "üåê Web Scraping", "üìã Cap√≠tulos", "üì¶ Download", "üìä M√©tricas"])

with tab1:
    modo = st.radio("**Escolha o modo:**", 
                   ["üì§ Upload", "üåê URLs"], horizontal=True)
    
    if modo == "üì§ Upload":
        arquivos = st.file_uploader(
            "Envie suas imagens:",
            type=["jpg", "jpeg", "png", "webp"],
            accept_multiple_files=True
        )
        
        if arquivos:
            # Sanitizar nomes de arquivo de upload
            files_data = []
            for f in arquivos:
                nome_limpo = input_validator.sanitize_filename(f.name)
                files_data.append((nome_limpo, f.read()))
                logger.info(f"Arquivo carregado: {f.name} -> {nome_limpo}")
            
            files_id = hash(tuple(f"{name}_{len(data)}" for name, data in files_data))
            
            if files_id not in st.session_state.imagens_processadas:
                st.session_state.imagens_processadas.add(files_id)
                
                progress_bar = st.progress(0)
                paineis_novos = []
                
                # Vers√£o ass√≠ncrona para uploads m√∫ltiplos
                async def process_uploads_async():
                    tasks = []
                    for nome, data in files_data:
                        task = async_manager.process_image_async(data, nome)
                        tasks.append((nome, task))
                    
                    for i, (nome, task) in enumerate(tasks):
                        progress_bar.progress((i + 1) / len(tasks))
                        
                        try:
                            paineis, erro = await task
                            if not erro:
                                paineis_novos.extend(paineis)
                                # Mostrar preview ap√≥s processamento
                                if paineis:
                                    mostrar_paineis_grid_otimizado(paineis, f"üìÑ {nome}")
                        except Exception as e:
                            logger.error(f"Erro no processamento ass√≠ncrono de {nome}: {e}")
                    
                    return paineis_novos
                
                # Executar processamento ass√≠ncrono
                try:
                    paineis_novos = run_async(process_uploads_async())
                    st.session_state.painel_coletor.extend(paineis_novos)
                except Exception as e:
                    logger.error(f"Erro no processamento ass√≠ncrono de uploads: {e}")
                    st.error(f"Erro no processamento: {e}")
                finally:
                    progress_bar.empty()
                
                if paineis_novos:
                    st.success(f"‚úÖ {len(paineis_novos)} pain√©is extra√≠dos!")

with tab2:
    st.markdown("### üåê Extra√ß√£o de Manhwa de Sites")
    st.info("üìñ Cole a URL da p√°gina principal do manhwa (ex: manhwatop.com, reaperscans.com, etc.)")
    
    url_manhwa = st.text_input(
        "URL do Manhwa:",
        placeholder="https://manhwatop.com/manga/nome-do-manhwa/",
        help="URL da p√°gina principal do manhwa, n√£o de um cap√≠tulo espec√≠fico"
    )
    
    if url_manhwa and st.button("üîç Analisar Manhwa", type="primary"):
        try:
            # Validar e sanitizar URL de entrada
            url_manhwa = input_validator.validate_url_input(url_manhwa)
            logger.info(f"URL validada com sucesso: {url_manhwa}")
            
            if not validar_url_cached(url_manhwa):
                st.error("‚ùå URL inv√°lida ou n√£o permitida!")
            else:
                with st.spinner("üîç Analisando manhwa..."):
                    soup = fazer_requisicao_web(url_manhwa)
                    if soup:
                        base_url = f"{urlparse(url_manhwa).scheme}://{urlparse(url_manhwa).netloc}"
                        
                        # Detectar tipo de site e usar extrator apropriado
                        if "manhwatop.com" in url_manhwa.lower():
                            info = extrair_info_manhwa_manhwatop(soup, base_url)
                        else:
                            # Extrator gen√©rico para outros sites
                            info = extrair_info_manhwa_manhwatop(soup, base_url)  # Usar o mesmo por enquanto
                        
                        if info["capitulos"]:
                            st.session_state.manhwa_info = info
                            st.session_state.capitulos_cache[url_manhwa] = info
                            st.success(f"‚úÖ Encontrados {len(info['capitulos'])} cap√≠tulos!")
                            st.rerun()
                        else:
                            st.error("‚ùå Nenhum cap√≠tulo encontrado. Verifique se a URL est√° correta.")
        except ValueError as e:
            st.error(f"‚ùå {e}")
            logger.warning(f"URL rejeitada: {url_manhwa} - {e}")
        except Exception as e:
            st.error("‚ùå Erro na valida√ß√£o da URL!")
            logger.error(f"Erro inesperado na valida√ß√£o de URL: {e}")

with tab3:
    if st.session_state.manhwa_info:
        info = st.session_state.manhwa_info
        
        # Mostrar informa√ß√µes do manhwa
        col1, col2 = st.columns([1, 3])
        
        with col1:
            if info.get("capa"):
                try:
                    capa_data = baixar_imagem_url_otimizada(info["capa"])
                    if capa_data:
                        capa_img = Image.open(io.BytesIO(capa_data))
                        st.image(capa_img, width=200)
                except (requests.RequestException, IOError, ValueError) as e:
                    st.write("üñºÔ∏è Capa n√£o dispon√≠vel")
                    logger.warning(f"Falha ao carregar imagem de capa: {e}")
        
        with col2:
            st.markdown(f"## üìö {info['titulo']}")
            if info.get("sinopse"):
                st.markdown(f"**Sinopse:** {info['sinopse']}")
            st.markdown(f"**Cap√≠tulos dispon√≠veis:** {len(info['capitulos'])}")
        
        st.markdown("---")
        st.markdown("### üìã Lista de Cap√≠tulos")
        
        # Filtros
        col1, col2, col3 = st.columns(3)
        with col1:
            filtro_inicio = st.number_input("Cap√≠tulo inicial:", min_value=1, value=1)
        with col2:
            filtro_fim = st.number_input("Cap√≠tulo final:", min_value=1, value=len(info['capitulos']))
        with col3:
            modo_exibicao = st.selectbox("Exibi√ß√£o:", ["Lista", "Grade"])
        
        # Filtrar cap√≠tulos
        capitulos_filtrados = [
            cap for cap in info['capitulos'] 
            if filtro_inicio <= float(cap['numero']) <= filtro_fim
        ]
        
        if modo_exibicao == "Lista":
            for cap in capitulos_filtrados[:20]:  # Limitar exibi√ß√£o
                col1, col2, col3 = st.columns([3, 1, 1])
                
                with col1:
                    st.markdown(f"**Cap. {cap['numero']}** - {cap['titulo']}")
                
                with col2:
                    if st.button(f"üëÅÔ∏è Ver", key=f"ver_{cap['numero']}"):
                        st.write(f"URL: {cap['url']}")
                
                with col3:
                    if st.button(f"üì• Baixar", key=f"download_{cap['numero']}", type="primary"):
                        with st.spinner(f"Processando Cap√≠tulo {cap['numero']}..."):
                            paineis_cap = processar_capitulo_completo(cap)
                            
                            if paineis_cap:
                                # Criar ZIP espec√≠fico do cap√≠tulo
                                zip_data = criar_zip_otimizado(paineis_cap)
                                
                                if zip_data:
                                    st.download_button(
                                        f"üì¶ Cap. {cap['numero']} ({len(paineis_cap)} pain√©is)",
                                        data=zip_data,
                                        file_name=f"{info['titulo']}_cap_{cap['numero']}.zip",
                                        mime="application/zip",
                                        key=f"zip_{cap['numero']}"
                                    )
                                    
                                    # Adicionar √† cole√ß√£o geral
                                    st.session_state.painel_coletor.extend(paineis_cap)
                                    st.success(f"‚úÖ {len(paineis_cap)} pain√©is extra√≠dos do Cap√≠tulo {cap['numero']}!")
                            else:
                                st.error(f"‚ùå Nenhum painel encontrado no Cap√≠tulo {cap['numero']}")
        else:
            # Modo grade
            cols = st.columns(3)
            for i, cap in enumerate(capitulos_filtrados[:15]):  # Limitar para n√£o sobrecarregar
                with cols[i % 3]:
                    st.markdown(f"**Cap. {cap['numero']}**")
                    st.markdown(f"{cap['titulo'][:30]}...")
                    
                    col_ver, col_down = st.columns(2)
                    with col_ver:
                        if st.button("üëÅÔ∏è", key=f"ver_grid_{cap['numero']}", help="Ver cap√≠tulo"):
                            st.write(f"URL: {cap['url']}")
                    
                    with col_down:
                        if st.button("üì•", key=f"down_grid_{cap['numero']}", help="Baixar cap√≠tulo", type="primary"):
                            with st.spinner(f"Processando..."):
                                paineis_cap = processar_capitulo_completo(cap)
                                
                                if paineis_cap:
                                    zip_data = criar_zip_otimizado(paineis_cap)
                                    
                                    if zip_data:
                                        st.download_button(
                                            f"üì¶ Download",
                                            data=zip_data,
                                            file_name=f"{info['titulo']}_cap_{cap['numero']}.zip",
                                            mime="application/zip",
                                            key=f"zip_grid_{cap['numero']}"
                                        )
                                        
                                        st.session_state.painel_coletor.extend(paineis_cap)
                                        st.success(f"‚úÖ {len(paineis_cap)} pain√©is!")
                                else:
                                    st.error("‚ùå Erro!")
        
        # Op√ß√£o para baixar m√∫ltiplos cap√≠tulos
        st.markdown("---")
        st.markdown("### üì¶ Download em Lote")
        
        col1, col2 = st.columns(2)
        with col1:
            range_inicio = st.number_input("Do cap√≠tulo:", min_value=1, value=1, key="range_inicio")
        with col2:
            range_fim = st.number_input("At√© o cap√≠tulo:", min_value=1, value=min(5, len(info['capitulos'])), key="range_fim")
        
        if st.button("üì• Baixar Cap√≠tulos em Lote", type="primary"):
            try:
                # Validar faixa de cap√≠tulos
                range_inicio_val, range_fim_val = input_validator.validate_chapter_range(
                    int(range_inicio), int(range_fim), max_chapters=50
                )
                
                if range_inicio_val != range_inicio or range_fim_val != range_fim:
                    st.warning(f"üìä Faixa ajustada para: {range_inicio_val} - {range_fim_val}")
                
                capitulos_selecionados = [
                    cap for cap in info['capitulos'] 
                    if range_inicio_val <= float(cap['numero']) <= range_fim_val
                ]
            except ValueError as e:
                st.error(f"‚ùå {e}")
                capitulos_selecionados = []
            
            if capitulos_selecionados:
                total_paineis = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, cap in enumerate(capitulos_selecionados):
                    status_text.text(f"Processando Cap√≠tulo {cap['numero']}...")
                    progress_bar.progress((i + 1) / len(capitulos_selecionados))
                    
                    paineis_cap = processar_capitulo_completo(cap)
                    total_paineis.extend(paineis_cap)
                    
                    # Smart delay entre cap√≠tulos para n√£o sobrecarregar o servidor
                    smart_sleep(context="batch_download", show_progress=True)
                
                progress_bar.empty()
                status_text.empty()
                
                if total_paineis:
                    st.session_state.painel_coletor.extend(total_paineis)
                    st.success(f"‚úÖ {len(total_paineis)} pain√©is extra√≠dos de {len(capitulos_selecionados)} cap√≠tulos!")
                    
                    # Criar ZIP do lote
                    zip_data = criar_zip_otimizado(total_paineis)
                    if zip_data:
                        st.download_button(
                            f"üì¶ Baixar Lote ({len(total_paineis)} pain√©is)",
                            data=zip_data,
                            file_name=f"{info['titulo']}_caps_{range_inicio}-{range_fim}.zip",
                            mime="application/zip"
                        )
                else:
                    st.error("‚ùå Nenhum painel foi extra√≠do!")
    else:
        st.info("üìã Primeiro, analise um manhwa na aba 'Web Scraping' para ver os cap√≠tulos dispon√≠veis.")

with tab4:
    if st.session_state.painel_coletor:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.success(f"‚úÖ **{len(st.session_state.painel_coletor)} pain√©is** prontos para download")
        
        with col2:
            zip_data = criar_zip_otimizado()
            if zip_data:
                st.download_button(
                    "üì¶ Baixar Todos os Pain√©is",
                    data=zip_data,
                    file_name="paineis_manhwa_completo.zip",
                    mime="application/zip",
                    type="primary",
                    use_container_width=True
                )
        
        # Mostrar pr√©via dos pain√©is
        st.markdown("### üñºÔ∏è Pr√©via dos Pain√©is")
        mostrar_paineis_grid_otimizado(st.session_state.painel_coletor, "Todos os Pain√©is", expandido=False)
        
        # Op√ß√µes adicionais
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üóëÔ∏è Limpar Cole√ß√£o", type="secondary"):
                st.session_state.painel_coletor.clear()
                st.success("Cole√ß√£o limpa!")
                st.rerun()
        
        with col2:
            # Estat√≠sticas
            st.metric("Total de Pain√©is", len(st.session_state.painel_coletor))
        
        with col3:
            # Tamanho estimado do ZIP
            tamanho_estimado = len(st.session_state.painel_coletor) * 0.5  # Estimativa de 500KB por painel
            st.metric("Tamanho Estimado", f"{tamanho_estimado:.1f} MB")
        
    else:
        st.info("üìã Nenhum painel extra√≠do ainda.")
        st.markdown("### üöÄ Como come√ßar:")
        st.markdown("""
        1. **üì§ Upload**: Envie suas pr√≥prias imagens de manhwa
        2. **üåê Web Scraping**: Cole a URL de um manhwa online
        3. **üìã Cap√≠tulos**: Selecione e baixe cap√≠tulos espec√≠ficos
        4. **üì¶ Download**: Baixe todos os pain√©is extra√≠dos
        """)

with tab5:
    st.markdown("# üìä Dashboard de Performance")
    st.markdown("Monitoramento em tempo real da aplica√ß√£o")
    
    # Bot√µes de controle
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üîÑ Atualizar M√©tricas", type="primary"):
            st.rerun()
    
    with col2:
        if st.button("üìä Exportar Relat√≥rio"):
            stats = metrics_collector.get_summary_stats()
            stats_json = json.dumps(stats, indent=2)
            st.download_button(
                "üì• Baixar JSON",
                data=stats_json,
                file_name=f"metrics_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    with col3:
        auto_refresh = st.checkbox("üîÑ Auto-refresh (10s)", help="Atualiza m√©tricas automaticamente")
    
    # Dashboard principal
    metrics_collector.display_dashboard()
    
    # Se√ß√£o de configura√ß√µes avan√ßadas
    with st.expander("‚öôÔ∏è Configura√ß√µes Avan√ßadas"):
        st.markdown("#### Limites de Alerta")
        
        col1, col2 = st.columns(2)
        with col1:
            success_threshold = st.slider("Taxa de sucesso m√≠nima (%)", 50, 100, 80)
            download_threshold = st.slider("Tempo m√°ximo de download (s)", 1, 20, 5)
        
        with col2:
            processing_threshold = st.slider("Tempo m√°ximo de processamento (s)", 1, 30, 10)
            memory_threshold = st.slider("Limite de mem√≥ria (MB)", 100, 2000, 1000)
        
        if st.button("üíæ Salvar Configura√ß√µes"):
            st.success("Configura√ß√µes salvas!")
    
    # Auto-refresh
    if auto_refresh:
        time.sleep(10)
        st.rerun()

# Rodap√© com informa√ß√µes adicionais
st.markdown("---")
st.markdown("### üìñ Sites Suportados")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    **üåü Totalmente Suportados:**
    - ManhwaTop.com
    - ReaperScans.com
    - AsuraScans.com
    """)

with col2:
    st.markdown("""
    **‚ö° Parcialmente Suportados:**
    - MangaDex.org
    - MangaPlus.com
    - Webtoons.com
    """)

with col3:
    st.markdown("""
    **üîß Em Desenvolvimento:**
    - Outros sites de manhwa
    - Melhorias na detec√ß√£o
    - Suporte a mais formatos
    """)

# Avisos importantes
st.markdown("---")
st.warning("""
‚ö†Ô∏è **Avisos Importantes:**
- Respeite os direitos autorais dos manhwas
- Use apenas para fins pessoais e educacionais
- Alguns sites podem ter prote√ß√µes anti-bot
- Velocidade de download depende do servidor de origem
""")

# CSS otimizado
st.markdown("""
<style>
.stApp { 
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
}
.main .block-container { 
    background: rgba(255, 255, 255, 0.95); 
    border-radius: 15px; 
    padding: 2rem; 
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
    backdrop-filter: blur(10px);
}
.stTabs [data-baseweb="tab-list"] {
    gap: 2px;
}
.stTabs [data-baseweb="tab"] {
    height: 50px;
    padding-left: 20px;
    padding-right: 20px;
    background-color: rgba(255, 255, 255, 0.1);
    border-radius: 10px 10px 0 0;
    color: #4a4a4a;
    font-weight: 500;
}
.stTabs [aria-selected="true"] {
    background-color: rgba(255, 255, 255, 0.9);
    color: #667eea;
}
.stButton > button {
    border-radius: 10px;
    border: none;
    padding: 0.5rem 1rem;
    font-weight: 500;
    transition: all 0.3s ease;
}
.stButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}
.stDownloadButton > button {
    background: linear-gradient(45deg, #667eea, #764ba2);
    color: white;
    border: none;
    border-radius: 10px;
    padding: 0.75rem 1.5rem;
    font-weight: 600;
    transition: all 0.3s ease;
}
.stDownloadButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
}
.stExpander {
    border: 1px solid rgba(255, 255, 255, 0.2);
    border-radius: 10px;
    background: rgba(255, 255, 255, 0.05);
    margin: 1rem 0;
}
.stMetric {
    background: rgba(255, 255, 255, 0.1);
    padding: 1rem;
    border-radius: 10px;
    border: 1px solid rgba(255, 255, 255, 0.2);
}
.stAlert {
    border-radius: 10px;
    border: none;
    padding: 1rem;
    margin: 1rem 0;
}
.stProgress .st-bo {
    background-color: rgba(102, 126, 234, 0.2);
}
.stProgress .st-bp {
    background: linear-gradient(45deg, #667eea, #764ba2);
}
</style>
""", unsafe_allow_html=True)
# --- CSS customizado para apar√™ncia melhor ---

st.markdown("""
<style>
h1, h2, h3 {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    color: #1f77b4;
}

body {
    background-color: #f0f2f6;
}

.stButton>button {
    background-color: #1f77b4;
    color: white;
    border-radius: 10px;
    padding: 8px 15px;
    font-weight: bold;
    border: none;
    cursor: pointer;
    transition: background-color 0.3s ease;
}

.stButton>button:hover {
    background-color: #145a86;
}

.stProgress > div > div > div > div {
    background-color: #1f77b4 !important;
}

.botao-flutuante-topo, .botao-flutuante-baixo {
    position: fixed;
    right: 30px;
    z-index: 9999;
    background-color: #1f77b4;
    color: white !important;
    padding: 12px 24px;
    border: none;
    border-radius: 8px;
    font-size: 16px;
    font-weight: bold;
    text-decoration: none;
    box-shadow: 0px 2px 6px rgba(0,0,0,0.3);
    cursor: pointer;
}
.botao-flutuante-topo {
    bottom: 100px;
}
.botao-flutuante-baixo {
    bottom: 40px;
}
</style>

<a href="#topo" class="botao-flutuante-topo">‚¨ÜÔ∏è Topo</a>
<a href="#final_paineis" class="botao-flutuante-baixo">‚¨áÔ∏è Pain√©is</a>
""", unsafe_allow_html=True)

st.markdown("""
<script>
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener("click", function(e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute("href"));
        if (target) {
            target.scrollIntoView({ behavior: "smooth", block: "start" });
        }
    });
});
</script>
""", unsafe_allow_html=True)
st.markdown('<div id="final_paineis" style="height:1px;"></div>', unsafe_allow_html=True)
